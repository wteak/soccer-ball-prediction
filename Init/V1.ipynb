{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f5a669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [High Accuracy] LSTM Sequence Model ---\n",
      " -> 사용 장치: cpu\n",
      "1. 데이터 로드 및 시퀀스 변환...\n",
      " -> 시퀀스 변환 중... (Train)\n",
      "2. 학습 시작 (Epochs: 50)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 206\u001b[39m\n\u001b[32m    204\u001b[39m outputs = model(padded_seqs, lengths)\n\u001b[32m    205\u001b[39m loss = criterion(outputs, targets)\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m optimizer.step()\n\u001b[32m    209\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wch23\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wch23\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wch23\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd#기본 simple MLP 모델 사용 \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# 경고 무시\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"--- [PyTorch Version] Episode-Based Prediction ---\")\n",
    "\n",
    "# ==============================================================================\n",
    "# [1] 설정 및 하이퍼파라미터\n",
    "# ==============================================================================\n",
    "TRAIN_FILE = 'train.csv'\n",
    "TEST_FOLDER = 'test/'\n",
    "SAMPLE_FILE = 'sample_submission.csv'\n",
    "OUTPUT_FILE = 'submission_torch.csv'\n",
    "\n",
    "# 하이퍼파라미터\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 30 # 학습 횟수\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\" -> 사용 장치: {DEVICE}\")\n",
    "\n",
    "if not os.path.exists(SAMPLE_FILE):\n",
    "    raise FileNotFoundError(f\"'{SAMPLE_FILE}' 파일이 없습니다.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# [2] 데이터 전처리\n",
    "# ==============================================================================\n",
    "def process_data(df, encoders=None, is_train=True):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. 거리/각도\n",
    "    df['dist_to_goal'] = np.sqrt((105 - df['start_x'])**2 + (34 - df['start_y'])**2)\n",
    "    df['angle_to_goal'] = np.arctan2((34 - df['start_y']), (105 - df['start_x']))\n",
    "    \n",
    "    # 2. 범주형 인코딩\n",
    "    cat_cols = ['type_name', 'result_name']\n",
    "    if is_train:\n",
    "        encoders = {}\n",
    "        for col in cat_cols:\n",
    "            le = LabelEncoder()\n",
    "            df[col] = df[col].fillna('Unknown').astype(str)\n",
    "            df[col] = le.fit_transform(df[col])\n",
    "            encoders[col] = le\n",
    "    else:\n",
    "        for col in cat_cols:\n",
    "            df[col] = df[col].fillna('Unknown').astype(str)\n",
    "            le = encoders[col]\n",
    "            df[col] = df[col].map(lambda x: le.transform([x])[0] if x in le.classes_ else -1)\n",
    "\n",
    "    # 3. Aggregation (에피소드 단위 통계)\n",
    "    num_aggs = {\n",
    "        'start_x': ['mean', 'std', 'min', 'max', 'last'],\n",
    "        'start_y': ['mean', 'std', 'min', 'max', 'last'],\n",
    "        'dist_to_goal': ['mean', 'last'],\n",
    "        'time_seconds': ['count', 'max', 'min']\n",
    "    }\n",
    "    cat_aggs = {\n",
    "        'type_name': ['nunique', 'last'],\n",
    "        'result_name': ['last']\n",
    "    }\n",
    "    \n",
    "    agg_dict = {**num_aggs, **cat_aggs}\n",
    "    \n",
    "    if is_train:\n",
    "        agg_dict['end_x'] = ['last']\n",
    "        agg_dict['end_y'] = ['last']\n",
    "\n",
    "    grp = df.groupby(['game_id', 'episode_id'])\n",
    "    df_agg = grp.agg(agg_dict)\n",
    "    \n",
    "    df_agg.columns = ['_'.join(col).strip() for col in df_agg.columns.values]\n",
    "    df_agg = df_agg.reset_index()\n",
    "    \n",
    "    # NaN 값 처리 (std 계산 등에서 발생 가능) - 0으로 채움\n",
    "    df_agg = df_agg.fillna(0)\n",
    "    \n",
    "    return df_agg, encoders\n",
    "\n",
    "# ==============================================================================\n",
    "# [3] PyTorch Dataset & Model 정의\n",
    "# ==============================================================================\n",
    "class SoccerDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y) if y is not None else None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        return self.X[idx]\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.BatchNorm1d(128), # 학습 안정화\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),     # 과적합 방지\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(64, 2)     # 출력: end_x, end_y\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ==============================================================================\n",
    "# [4] 데이터 준비 및 스케일링\n",
    "# ==============================================================================\n",
    "print(\"1. 데이터 로드 및 전처리...\")\n",
    "df_train = pd.read_csv(TRAIN_FILE)\n",
    "train_agg, saved_encoders = process_data(df_train, is_train=True)\n",
    "\n",
    "# Target 설정\n",
    "target_cols = ['end_x_last', 'end_y_last']\n",
    "features = [c for c in train_agg.columns if c not in ['game_id', 'episode_id'] + target_cols]\n",
    "\n",
    "# *** 중요: 딥러닝을 위한 스케일링 (StandardScaler) ***\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_agg[features])\n",
    "y_train = train_agg[target_cols].values\n",
    "\n",
    "# Dataset & DataLoader 생성\n",
    "train_dataset = SoccerDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 모델 초기화\n",
    "model = SimpleMLP(input_dim=len(features)).to(DEVICE)\n",
    "criterion = nn.MSELoss() # 회귀 문제이므로 평균제곱오차 사용\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# ==============================================================================\n",
    "# [5] 모델 학습\n",
    "# ==============================================================================\n",
    "print(f\"2. 모델 학습 시작 (Epochs: {EPOCHS})...\")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"   Epoch [{epoch+1}/{EPOCHS}], Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# [6] Test 예측 및 결과 저장\n",
    "# ==============================================================================\n",
    "print(\"3. Test 데이터 예측...\")\n",
    "\n",
    "# Test 파일 로드\n",
    "test_files = []\n",
    "for root, dirs, files in os.walk(TEST_FOLDER):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            test_files.append(os.path.join(root, file))\n",
    "\n",
    "df_test_list = [pd.read_csv(f) for f in test_files]\n",
    "df_test = pd.concat(df_test_list, ignore_index=True)\n",
    "\n",
    "# 전처리\n",
    "test_agg, _ = process_data(df_test, encoders=saved_encoders, is_train=False)\n",
    "\n",
    "# 스케일링 (Train 기준 적용)\n",
    "X_test = scaler.transform(test_agg[features])\n",
    "\n",
    "# 예측\n",
    "test_dataset = SoccerDataset(X_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(DEVICE)\n",
    "        outputs = model(X_batch)\n",
    "        all_preds.append(outputs.cpu().numpy())\n",
    "\n",
    "predictions = np.vstack(all_preds)\n",
    "\n",
    "# Clipping\n",
    "MAX_X, MAX_Y = 105.0, 68.0\n",
    "test_agg['pred_end_x'] = np.clip(predictions[:, 0], 0, MAX_X)\n",
    "test_agg['pred_end_y'] = np.clip(predictions[:, 1], 0, MAX_Y)\n",
    "\n",
    "# ==============================================================================\n",
    "# [7] Sample Submission 매핑 (절대 기준)\n",
    "# ==============================================================================\n",
    "print(\"4. Sample Submission 파일 병합...\")\n",
    "\n",
    "submission = pd.read_csv(SAMPLE_FILE)\n",
    "\n",
    "# Key 분리 (153363_1 -> 153363, 1)\n",
    "submission['game_id'] = submission['game_episode'].apply(lambda x: int(x.split('_')[0]))\n",
    "submission['episode_id'] = submission['game_episode'].apply(lambda x: int(x.split('_')[1]))\n",
    "\n",
    "# 병합\n",
    "final_df = pd.merge(submission, test_agg[['game_id', 'episode_id', 'pred_end_x', 'pred_end_y']], \n",
    "                    on=['game_id', 'episode_id'], \n",
    "                    how='left')\n",
    "\n",
    "# 컬럼 정리\n",
    "final_df['end_x'] = final_df['pred_end_x']\n",
    "final_df['end_y'] = final_df['pred_end_y']\n",
    "final_df = final_df.fillna(0)\n",
    "\n",
    "# 최종 저장\n",
    "final_output = final_df[['game_episode', 'end_x', 'end_y']]\n",
    "final_output.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f\"\\n[성공] '{OUTPUT_FILE}' 생성 완료.\")\n",
    "print(f\" -> 데이터 확인:\\n{final_output.head()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
